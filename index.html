<!DOCTYPE HTML>
<!--
	Visualize by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<title>PoseNet Project</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<span class="avatar"><img src="images/avatar.jpg" alt="" /></span>
						<span class="avatar"><img src="images/ava.jpg" alt="" /></span>
						<h1>This project was created to display a machine learning model that can detect if a user is falling asleep.</h1>
						<h2>The model will play one of two different .mp3 files determined by the user tilting their head to either the left or right.</h2>
					</header>

				<!-- Main -->
					<section id="main">

						<!-- Thumbnails -->
						<center>
						<div>Teachable Machine Pose Model</div>
						<button type="button" onclick="init()">Start</button>
						<div><canvas id="canvas"></canvas></div>
						<div id="label-container"></div>
						<br><br><br><br>
						<audio id="left">
						  <source src="Left.mp3" type="audio/mpeg">
						  Your browser does not support HTML5 audio.
						</audio>
						<audio id="right">
						  <source src="Right.mp3" type="audio/mpeg">
						  Your browser does not support HTML5 audio.
						</audio>	
						</center>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/main.js"></script>
			<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
			<script src="https://cdn.jsdelivr.net/npm/@teachablemachine/pose@0.8/dist/teachablemachine-pose.min.js"></script>
			<script type="text/javascript">
				// More API functions here:
				// https://github.com/googlecreativelab/teachablemachine-community/tree/master/libraries/pose

				// the link to your model provided by Teachable Machine export panel
				const URL = "https://teachablemachine.withgoogle.com/models/OZk85KQkW/";
				let model, webcam, ctx, labelContainer, maxPredictions;

				// functions to play and pause audio file
				var aud = document.getElementById("left");
				var aud1 = document.getElementById("right"); 

				function playAud() { 
				  aud.play(); 
				} 

				function pauseAud() { 
				  aud.pause(); 
				} 
				
				function playAud1() { 
				  aud1.play(); 
				} 

				function pauseAud1() { 
				  aud1.pause(); 
				} 
				
				async function init() {
					const modelURL = URL + "model.json";
					const metadataURL = URL + "metadata.json";

					// load the model and metadata
					// Refer to tmImage.loadFromFiles() in the API to support files from a file picker
					// Note: the pose library adds a tmPose object to your window (window.tmPose)
					model = await tmPose.load(modelURL, metadataURL);
					maxPredictions = model.getTotalClasses();

					// Convenience function to setup a webcam
					const size = 400;
					const flip = true; // whether to flip the webcam
					webcam = new tmPose.Webcam(size, size, flip); // width, height, flip
					await webcam.setup(); // request access to the webcam
					await webcam.play();
					window.requestAnimationFrame(loop);

					// append/get elements to the DOM
					const canvas = document.getElementById("canvas");
					canvas.width = size; canvas.height = size;
					ctx = canvas.getContext("2d");
					labelContainer = document.getElementById("label-container");
					for (let i = 0; i < maxPredictions; i++) { // and class labels
						labelContainer.appendChild(document.createElement("div"));
					}
				}

				async function loop(timestamp) {
					webcam.update(); // update the webcam frame
					await predict();
					window.requestAnimationFrame(loop);
				}

				async function predict() {
					// Prediction #1: run input through posenet
					// estimatePose can take in an image, video or canvas html element
					const { pose, posenetOutput } = await model.estimatePose(webcam.canvas);
					// Prediction 2: run input through teachable machine classification model
					const prediction = await model.predict(posenetOutput);

					for (let i = 0; i < maxPredictions; i++) {
						const classPrediction =
							prediction[i].className + ": " + prediction[i].probability.toFixed(2);
						labelContainer.childNodes[i].innerHTML = classPrediction;
						
						// don't play audio when head's neutral with probability >= 75%
						if (prediction[1].probability.toFixed(2) >= 0.75)
							playAud();
						else
							pauseAud();
							
						if (prediction[2].probability.toFixed(2) >= 0.75)
							playAud1();
						else
							pauseAud1();
					}

					// finally draw the poses
					drawPose(pose);
				}

				function drawPose(pose) {
					if (webcam.canvas) {
						ctx.drawImage(webcam.canvas, 0, 0);
						// draw the keypoints and skeleton
						if (pose) {
							const minPartConfidence = 0.5;
							tmPose.drawKeypoints(pose.keypoints, minPartConfidence, ctx);
							tmPose.drawSkeleton(pose.keypoints, minPartConfidence, ctx);
						}
					}
				}
			</script>

	</body>
</html>

